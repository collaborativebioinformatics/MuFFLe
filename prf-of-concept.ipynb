{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c3d67-a6ea-4f59-84d2-effc3ef016e1",
   "metadata": {},
   "source": [
    "# Getting Started with NVFlare (PyTorch)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/NVFlare/blob/main/examples/hello-world/hello-pt/hello-pt.ipynb)\n",
    "\n",
    "NVFlare is an open-source framework that allows researchers and\n",
    "data scientists to seamlessly move their machine learning and deep\n",
    "learning workflows into a federated paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cb248-dc6a-48d1-880d-33c4324d9723",
   "metadata": {},
   "source": [
    "## Federated Averaging with NVFlare\n",
    "Given the flexible controller and executor concepts, it is easy to implement different computing & communication patterns with NVFlare, such as [FedAvg](https://proceedings.mlr.press/v54/mcmahan17a?ref=https://githubhelp.com) and [cyclic weight transfer](https://academic.oup.com/jamia/article/25/8/945/4956468). \n",
    "\n",
    "The controller's `run()` routine is responsible for assigning tasks and processing task results from the Executors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f84fb1-9dd3-4c72-a727-c4614260f02f",
   "metadata": {},
   "source": [
    "### Server Code\n",
    "In federated averaging, the server code is responsible for distributing the global model and aggregating model updates from clients. \n",
    "\n",
    "First, we provide a robust implementation of the [FedAvg](https://proceedings.mlr.press/v54/mcmahan17a?ref=https://githubhelp.com) algorithm with NVFlare. \n",
    "\n",
    "The server implements these main steps:\n",
    "1. FL server initializes an initial model.\n",
    "2. For each round (global iteration):\n",
    "    - FL server samples available clients.\n",
    "    - FL server sends the global model to clients and waits for their updates.\n",
    "    - FL server aggregates all the `results` and produces a new global model.\n",
    "\n",
    "In this example, we will directly use the default federated averaging algorithm provided by NVFlare utilizing the [FedAvgRecipe](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.app_opt.pt.recipes.fedavg.html#nvflare.app_opt.pt.recipes.fedavg.FedAvgRecipe) for PyTorch. \n",
    "\n",
    "There is no need to defined a customized server code for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b6476-089a-4e9d-825b-07107bd5d84a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Client Code \n",
    "We take a CIFAR-10 example directly from [PyTorch website](https://github.com/pytorch/tutorials/blob/main/beginner_source/blitz/cifar10_tutorial.py) with some minor modifications, such as removing comments, move the network to [src/net.py](src/net.py), and add a main method and GPU support. The original code can be found at [cifar10_original.py](../../hello-world/ml-to-fl/pt/code/cifar10_original.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c551053-5460-4d83-8578-796074170342",
   "metadata": {},
   "source": [
    "Now, we need to adapt this centralized training code to something that can run in a federated setting.\n",
    "\n",
    "On the client side, the training workflow is as follows:\n",
    "1. Receive the model from the FL server.\n",
    "2. Perform local training on the received global model\n",
    "and/or evaluate the received global model for model\n",
    "selection.\n",
    "3. Send the new model back to the FL server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bfc2a-783c-494f-9427-c38f40a2e870",
   "metadata": {},
   "source": [
    "Using NVFlare's client API, we can easily adapt machine learning code that was written for centralized training and apply it in a federated scenario.\n",
    "For a general use case, there are three essential methods to achieve this using the Client API :\n",
    "- `init()`: Initializes NVFlare Client API environment.\n",
    "- `receive()`: Receives model from the FL server.\n",
    "- `send()`: Sends the model to the FL server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da34414-bac4-4352-8077-ab7ade998eec",
   "metadata": {},
   "source": [
    "## Run an NVFlare Job\n",
    "Now that we have defined the FedAvg controller to run our federated compute workflow on the FL server, and our client training script to receive the global models, run local training, and send the results back to the FL server, we can put everything together using NVFlare's Job API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70da5d-ba8b-4e65-b47f-44bb9bddae4d",
   "metadata": {},
   "source": [
    "#### 2. Define a FedJob Recipe\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13771bfb-901f-485a-9a23-84db1ccd5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SimpleNetwork\n",
    "\n",
    "from nvflare.app_opt.pt.recipes.fedavg import FedAvgRecipe\n",
    "from nvflare.recipe import SimEnv\n",
    "# from nvflare.recipe import add_experiment_tracking\n",
    "import torch \n",
    "n_clients = 2\n",
    "num_rounds = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleNetwork(rna_dim=19359, clinical_dim=13).to(device)\n",
    "\n",
    "recipe = FedAvgRecipe(\n",
    "    name=\"hello-multimodal\",\n",
    "    min_clients=n_clients,\n",
    "    num_rounds=num_rounds,\n",
    "    initial_model=model,\n",
    "    train_script=\"client.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155da08-8244-457f-ab8a-ec8cba453732",
   "metadata": {},
   "source": [
    "#### 3. Add experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840d4b78-a4ec-4dec-bf70-673b1af07ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_experiment_tracking(recipe, tracking_type=\"tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3f0a8-06bb-4bea-89d3-4a5fc5b76c63",
   "metadata": {},
   "source": [
    "#### 4. Run Job\n",
    "Here, we run the job in a simulation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13068ab7-35cf-49e7-91ed-10993049ef0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38m2026-01-08 12:11:29,553 - INFO - model selection weights control: {}\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:30,147 - INFO - Tensorboard records can be found in /tmp/nvflare/simulation/hello-multimodal/server/simulate_job/tb_events you can view it using `tensorboard --logdir=/tmp/nvflare/simulation/hello-multimodal/server/simulate_job/tb_events`\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:30,147 - INFO - Initializing ScatterAndGather workflow for Federated Averaging.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:30,147 - INFO - Both source_ckpt_file_full_name and ckpt_preload_path are not provided. Using the default model weights initialized on the persistor side.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:30,147 - INFO - Beginning ScatterAndGather training phase.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:30,148 - INFO - Round 0 started.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:33,794 - INFO - start task run() with full path: /tmp/nvflare/simulation/hello-multimodal/site-1/simulate_job/app_site-1/custom/client.py\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:33,794 - INFO - start task run() with full path: /tmp/nvflare/simulation/hello-multimodal/site-2/simulate_job/app_site-2/custom/client.py\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,112 - INFO - execute for task (train)\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,112 - INFO - send data to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,112 - INFO - sending payload to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,113 - INFO - Waiting for result from peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,138 - INFO - execute for task (train)\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,139 - INFO - send data to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,139 - INFO - sending payload to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,140 - INFO - Waiting for result from peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,365 - INFO - --- site-1: Training CLINICAL modality ---\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,365 - INFO - site = site-1, current_round=0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,371 - INFO - --- site-2: Training RNA modality ---\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,371 - INFO - site = site-2, current_round=0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,382 - INFO - [1,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,382 - INFO - site=site-1, Epoch: 0/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,384 - INFO - [1,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,384 - INFO - site=site-1, Epoch: 0/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,386 - INFO - [1,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,386 - INFO - site=site-1, Epoch: 0/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,388 - INFO - [1,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,388 - INFO - site=site-1, Epoch: 0/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,390 - INFO - [1,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,391 - INFO - site=site-1, Epoch: 0/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,392 - INFO - [1,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,392 - INFO - site=site-1, Epoch: 0/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,394 - INFO - [2,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,394 - INFO - site=site-1, Epoch: 1/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,396 - INFO - [2,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,396 - INFO - site=site-1, Epoch: 1/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,398 - INFO - [2,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,399 - INFO - site=site-1, Epoch: 1/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,400 - INFO - [2,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,400 - INFO - site=site-1, Epoch: 1/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,401 - INFO - [2,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,401 - INFO - site=site-1, Epoch: 1/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,402 - INFO - [2,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,403 - INFO - site=site-1, Epoch: 1/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,403 - INFO - Finished Training for site-1\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,544 - INFO - [1,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,545 - INFO - site=site-2, Epoch: 0/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,594 - INFO - site: site-1, sending model to server.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,714 - INFO - [1,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,715 - INFO - site=site-2, Epoch: 0/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:34,822 - WARNING - Aggregation_weight missing for site-1 and set to default value, 1.0 This kind of message will show 10 times at most.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,845 - INFO - Contribution from site-1 ACCEPTED by the aggregator at round 0.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,888 - INFO - [1,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:34,889 - INFO - site=site-2, Epoch: 0/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,043 - INFO - [1,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,043 - INFO - site=site-2, Epoch: 0/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,194 - INFO - [1,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,195 - INFO - site=site-2, Epoch: 0/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,285 - INFO - [1,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,286 - INFO - site=site-2, Epoch: 0/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,438 - INFO - [2,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,439 - INFO - site=site-2, Epoch: 1/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,589 - INFO - [2,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,589 - INFO - site=site-2, Epoch: 1/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,742 - INFO - [2,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,743 - INFO - site=site-2, Epoch: 1/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,892 - INFO - [2,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:35,893 - INFO - site=site-2, Epoch: 1/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:36,041 - INFO - [2,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:36,042 - INFO - site=site-2, Epoch: 1/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:36,128 - INFO - [2,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:36,129 - INFO - site=site-2, Epoch: 1/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:36,129 - INFO - Finished Training for site-2\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:36,308 - INFO - site: site-2, sending model to server.\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:36,866 - WARNING - Aggregation_weight missing for site-2 and set to default value, 1.0 This kind of message will show 10 times at most.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:36,892 - INFO - Contribution from site-2 ACCEPTED by the aggregator at round 0.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,091 - INFO - Start aggregation.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,091 - INFO - aggregating 2 update(s) at round 0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,110 - INFO - End aggregation.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,111 - INFO - Start persist model on server.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,312 - INFO - End persist model on server.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,312 - INFO - Round 0 finished.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,337 - INFO - Round 1 started.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,599 - INFO - execute for task (train)\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,599 - INFO - send data to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,599 - INFO - sending payload to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,600 - INFO - Waiting for result from peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,628 - INFO - site = site-1, current_round=1\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,634 - INFO - [1,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,634 - INFO - site=site-1, Epoch: 0/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,635 - INFO - [1,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,636 - INFO - site=site-1, Epoch: 0/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,637 - INFO - [1,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,637 - INFO - site=site-1, Epoch: 0/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,639 - INFO - [1,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,639 - INFO - site=site-1, Epoch: 0/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,640 - INFO - [1,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,641 - INFO - site=site-1, Epoch: 0/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,642 - INFO - [1,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,642 - INFO - site=site-1, Epoch: 0/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,643 - INFO - [2,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,644 - INFO - site=site-1, Epoch: 1/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,645 - INFO - [2,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,645 - INFO - site=site-1, Epoch: 1/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,646 - INFO - [2,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,646 - INFO - site=site-1, Epoch: 1/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,647 - INFO - [2,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,651 - INFO - site=site-1, Epoch: 1/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,652 - INFO - [2,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,652 - INFO - site=site-1, Epoch: 1/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,653 - INFO - [2,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,654 - INFO - site=site-1, Epoch: 1/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,654 - INFO - Finished Training for site-1\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:37,757 - INFO - site: site-1, sending model to server.\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:38,330 - WARNING - validation metric not existing in site-1\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:38,331 - WARNING - Aggregation_weight missing for site-1 and set to default value, 1.0 This kind of message will show 10 times at most.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:38,348 - INFO - Contribution from site-1 ACCEPTED by the aggregator at round 1.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,176 - INFO - execute for task (train)\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,176 - INFO - send data to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,176 - INFO - sending payload to peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,177 - INFO - Waiting for result from peer\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,338 - INFO - site = site-2, current_round=1\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,509 - INFO - [1,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,510 - INFO - site=site-2, Epoch: 0/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,665 - INFO - [1,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,666 - INFO - site=site-2, Epoch: 0/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,820 - INFO - [1,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,821 - INFO - site=site-2, Epoch: 0/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,971 - INFO - [1,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:39,972 - INFO - site=site-2, Epoch: 0/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,122 - INFO - [1,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,123 - INFO - site=site-2, Epoch: 0/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,210 - INFO - [1,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,211 - INFO - site=site-2, Epoch: 0/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,364 - INFO - [2,     1] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,364 - INFO - site=site-2, Epoch: 1/2, Iteration: 0, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,515 - INFO - [2,     2] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,515 - INFO - site=site-2, Epoch: 1/2, Iteration: 1, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,683 - INFO - [2,     3] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,683 - INFO - site=site-2, Epoch: 1/2, Iteration: 2, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,833 - INFO - [2,     4] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,834 - INFO - site=site-2, Epoch: 1/2, Iteration: 3, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,987 - INFO - [2,     5] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:40,988 - INFO - site=site-2, Epoch: 1/2, Iteration: 4, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,076 - INFO - [2,     6] loss: 0.000\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,077 - INFO - site=site-2, Epoch: 1/2, Iteration: 5, Loss: 0.0\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,078 - INFO - Finished Training for site-2\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,184 - INFO - site: site-2, sending model to server.\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:41,396 - WARNING - validation metric not existing in site-2\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:41,397 - WARNING - Aggregation_weight missing for site-2 and set to default value, 1.0 This kind of message will show 10 times at most.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,439 - INFO - Contribution from site-2 ACCEPTED by the aggregator at round 1.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,466 - INFO - Start aggregation.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,472 - INFO - aggregating 2 update(s) at round 1\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,486 - INFO - End aggregation.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,486 - INFO - Start persist model on server.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,592 - INFO - End persist model on server.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,592 - INFO - Round 1 finished.\u001b[0m\n",
      "\u001b[38m2026-01-08 12:11:41,616 - INFO - Finished ScatterAndGather Training.\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:43,486 - WARNING - ask to stop job: reason: END_RUN received\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:43,487 - WARNING - ask to stop job: reason: END_RUN received\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:43,706 - WARNING - request to stop the job for reason END_RUN received\u001b[0m\n",
      "\u001b[33m2026-01-08 12:11:43,812 - WARNING - request to stop the job for reason END_RUN received\u001b[0m\n",
      "\n",
      "Note, get_status returns None in SimEnv. The simulation logs can be found at /tmp/nvflare/simulation/hello-multimodal\n",
      "Job Status is: None\n",
      "Result can be found in : /tmp/nvflare/simulation/hello-multimodal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimEnv(num_clients=n_clients)\n",
    "run = recipe.execute(env)\n",
    "print()\n",
    "print(\"Job Status is:\", run.get_status())\n",
    "print(\"Result can be found in :\", run.get_result())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6628eb-ce45-4bf8-90c9-1d8061ce048e",
   "metadata": {},
   "source": [
    "#### 5. Visualize the Training\n",
    "You can use TensorBoard to show the experiment tracking curves by running\n",
    "\n",
    "```bash\n",
    "tensorboard --bind_all --logdir /tmp/nvflare/simulation/hello-pt\n",
    "```\n",
    "in another terminal or directly show the training curves in the next notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f06f0c-a7b0-488c-ac54-35d99e379bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 21556), started 0:04:02 ago. (Use '!kill 21556' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2101c2a133eea45\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2101c2a133eea45\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asked Holger, logs are in server/simulate_job/tb_events\n",
    "# they were empty because of the 2000 datapt condition.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --bind_all --logdir /tmp/nvflare/simulation/hello-multimodal # also had this stuck as hello-pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Knowledge_Structures_Multimodal (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
